#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Detector de archivos duplicados basado en hash y tama√±o
"""

import hashlib
import os
import json
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional, Any
import logging
from datetime import datetime
from collections import defaultdict

logger = logging.getLogger(__name__)

class DetectorDuplicados:
    """
    Detecta archivos duplicados usando hash MD5/SHA256 y tama√±o.
    """
    
    def __init__(self, carpeta_descargas: Path):
        self.carpeta_descargas = carpeta_descargas
        self.carpeta_config = carpeta_descargas / ".config"
        self.carpeta_config.mkdir(exist_ok=True)
        self.archivo_cache = self.carpeta_config / "cache_duplicados.json"
        self.archivo_duplicados = self.carpeta_config / "duplicados_encontrados.json"
        self.cache_hashes: Dict[str, Dict[str, Any]] = {}
        self.duplicados_encontrados: List[Dict[str, Any]] = []
        self.algoritmo_hash = 'md5'  # 'md5' o 'sha256'
        self.usar_cache = True
        self._cargar_cache()
    
    def calcular_hash_archivo(self, archivo: Path) -> Optional[str]:
        """
        Calcula el hash de un archivo.
        
        Args:
            archivo: Ruta al archivo
            
        Returns:
            Hash del archivo o None si hay error
        """
        # Verificar cache primero
        ruta_str = str(archivo)
        if self.usar_cache and ruta_str in self.cache_hashes:
            info_cache = self.cache_hashes[ruta_str]
            try:
                # Verificar si el archivo ha cambiado
                stat = archivo.stat()
                if (info_cache['tama√±o'] == stat.st_size and 
                    info_cache['modificado'] == stat.st_mtime):
                    return info_cache['hash']
            except:
                pass
        
        try:
            # Calcular hash
            if self.algoritmo_hash == 'md5':
                hasher = hashlib.md5()
            else:
                hasher = hashlib.sha256()
            
            with open(archivo, 'rb') as f:
                # Leer en chunks para archivos grandes
                while chunk := f.read(8192):
                    hasher.update(chunk)
            
            hash_resultado = hasher.hexdigest()
            
            # Guardar en cache
            if self.usar_cache:
                stat = archivo.stat()
                self.cache_hashes[ruta_str] = {
                    'hash': hash_resultado,
                    'tama√±o': stat.st_size,
                    'modificado': stat.st_mtime,
                    'calculado': datetime.now().isoformat()
                }
            
            return hash_resultado
            
        except (IOError, PermissionError) as e:
            logger.warning(f"No se pudo calcular hash de {archivo}: {e}")
            return None
        except Exception as e:
            logger.error(f"Error calculando hash de {archivo}: {e}")
            return None
    
    def _cargar_cache(self):
        """Carga el cache de hashes calculados anteriormente."""
        if not self.archivo_cache.exists():
            return
        
        try:
            with open(self.archivo_cache, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.cache_hashes = data.get('hashes', {})
            self.algoritmo_hash = data.get('algoritmo', 'md5')
            
            logger.info(f"üì¶ Cache cargado: {len(self.cache_hashes)} hashes")
            
        except Exception as e:
            logger.error(f"Error cargando cache de duplicados: {e}")
    
    def _guardar_cache(self):
        """Guarda el cache de hashes."""
        try:
            # Limpiar entradas obsoletas (archivos que ya no existen)
            cache_limpio = {}
            for ruta, info in self.cache_hashes.items():
                if Path(ruta).exists():
                    cache_limpio[ruta] = info
            
            data = {
                'algoritmo': self.algoritmo_hash,
                'ultima_actualizacion': datetime.now().isoformat(),
                'hashes': cache_limpio
            }
            
            with open(self.archivo_cache, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            self.cache_hashes = cache_limpio
            
        except Exception as e:
            logger.error(f"Error guardando cache de duplicados: {e}")
    
    def _guardar_duplicados(self):
        """Guarda la lista de duplicados encontrados."""
        try:
            data = {
                'fecha_escaneo': datetime.now().isoformat(),
                'algoritmo_usado': self.algoritmo_hash,
                'total_grupos': len(self.duplicados_encontrados),
                'duplicados': self.duplicados_encontrados
            }
            
            with open(self.archivo_duplicados, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
        except Exception as e:
            logger.error(f"Error guardando duplicados: {e}")
    
    def escanear_duplicados(self, incluir_subcarpetas: bool = True, 
                          tama√±o_minimo: int = 1024) -> Dict[str, Any]:
        """
        Escanea la carpeta en busca de archivos duplicados.
        
        Args:
            incluir_subcarpetas: Si incluir subcarpetas en el escaneo
            tama√±o_minimo: Tama√±o m√≠nimo en bytes para considerar archivo
            
        Returns:
            Diccionario con resultados del escaneo
        """
        logger.info("üîç Iniciando escaneo de duplicados...")
        
        # Mapas para agrupar archivos
        archivos_por_tama√±o: Dict[int, List[Path]] = defaultdict(list)
        archivos_por_hash: Dict[str, List[Path]] = defaultdict(list)
        
        # Contador de progreso
        archivos_escaneados = 0
        archivos_procesados = 0
        
        # Primera pasada: agrupar por tama√±o
        patron = "**/*" if incluir_subcarpetas else "*"
        for archivo in self.carpeta_descargas.glob(patron):
            if not archivo.is_file():
                continue
            
            archivos_escaneados += 1
            
            try:
                tama√±o = archivo.stat().st_size
                if tama√±o >= tama√±o_minimo:
                    archivos_por_tama√±o[tama√±o].append(archivo)
            except (OSError, IOError):
                continue
        
        logger.info(f"üìä Primera pasada: {archivos_escaneados} archivos encontrados")
        
        # Segunda pasada: calcular hashes solo para archivos con mismo tama√±o
        grupos_duplicados = []
        
        for tama√±o, archivos in archivos_por_tama√±o.items():
            if len(archivos) < 2:
                continue  # Solo un archivo de este tama√±o
            
            logger.info(f"üîç Calculando hashes para {len(archivos)} archivos de {self._formatear_bytes(tama√±o)}")
            
            # Calcular hashes para archivos con el mismo tama√±o
            for archivo in archivos:
                hash_archivo = self.calcular_hash_archivo(archivo)
                if hash_archivo:
                    archivos_por_hash[hash_archivo].append(archivo)
                    archivos_procesados += 1
        
        # Identificar grupos de duplicados
        for hash_valor, archivos in archivos_por_hash.items():
            if len(archivos) > 1:
                # Encontramos duplicados
                grupo = {
                    'hash': hash_valor,
                    'tama√±o': archivos[0].stat().st_size,
                    'cantidad': len(archivos),
                    'archivos': []
                }
                
                for archivo in archivos:
                    try:
                        stat = archivo.stat()
                        info_archivo = {
                            'ruta': str(archivo),
                            'nombre': archivo.name,
                            'tama√±o': stat.st_size,
                            'modificado': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                            'carpeta': str(archivo.parent)
                        }
                        grupo['archivos'].append(info_archivo)
                    except:
                        continue
                
                if len(grupo['archivos']) > 1:
                    grupos_duplicados.append(grupo)
        
        # Ordenar por tama√±o (duplicados m√°s grandes primero)
        grupos_duplicados.sort(key=lambda g: g['tama√±o'], reverse=True)
        
        self.duplicados_encontrados = grupos_duplicados
        self._guardar_duplicados()
        self._guardar_cache()
        
        # Calcular estad√≠sticas
        total_duplicados = sum(g['cantidad'] - 1 for g in grupos_duplicados)  # -1 porque uno no es duplicado
        espacio_desperdiciado = sum(g['tama√±o'] * (g['cantidad'] - 1) for g in grupos_duplicados)
        
        resultado = {
            'archivos_escaneados': archivos_escaneados,
            'archivos_procesados': archivos_procesados,
            'grupos_duplicados': len(grupos_duplicados),
            'total_duplicados': total_duplicados,
            'espacio_desperdiciado': espacio_desperdiciado,
            'espacio_desperdiciado_legible': self._formatear_bytes(espacio_desperdiciado),
            'duplicados': grupos_duplicados
        }
        
        logger.info(f"‚úÖ Escaneo completado: {len(grupos_duplicados)} grupos, {total_duplicados} duplicados")
        logger.info(f"üíæ Espacio desperdiciado: {self._formatear_bytes(espacio_desperdiciado)}")
        
        return resultado
    
    def eliminar_duplicados(self, estrategia: str = 'mas_nuevo', 
                          confirmar: bool = False) -> Dict[str, Any]:
        """
        Elimina archivos duplicados seg√∫n una estrategia.
        
        Args:
            estrategia: 'mas_nuevo', 'mas_viejo', 'carpeta_principal', 'manual'
            confirmar: Si True, ejecuta la eliminaci√≥n. Si False, solo simula.
            
        Returns:
            Diccionario con resultados de la eliminaci√≥n
        """
        if not self.duplicados_encontrados:
            return {
                'exito': False,
                'mensaje': 'No hay duplicados para eliminar. Ejecuta escaneo primero.',
                'archivos_eliminados': 0
            }
        
        archivos_a_eliminar = []
        archivos_a_conservar = []
        
        for grupo in self.duplicados_encontrados:
            archivos = grupo['archivos']
            if len(archivos) < 2:
                continue
            
            # Seleccionar archivo a conservar seg√∫n estrategia
            if estrategia == 'mas_nuevo':
                conservar = max(archivos, key=lambda a: a['modificado'])
            elif estrategia == 'mas_viejo':
                conservar = min(archivos, key=lambda a: a['modificado'])
            elif estrategia == 'carpeta_principal':
                # Conservar el que est√© en la carpeta m√°s cercana a la ra√≠z
                conservar = min(archivos, key=lambda a: len(Path(a['ruta']).parts))
            else:
                # Estrategia manual - conservar el primero por defecto
                conservar = archivos[0]
            
            archivos_a_conservar.append(conservar)
            
            # Marcar el resto para eliminar
            for archivo in archivos:
                if archivo != conservar:
                    archivos_a_eliminar.append(archivo)
        
        # Ejecutar eliminaci√≥n si se confirma
        eliminados = 0
        errores = []
        espacio_liberado = 0
        
        if confirmar:
            for archivo_info in archivos_a_eliminar:
                try:
                    archivo = Path(archivo_info['ruta'])
                    if archivo.exists():
                        espacio_liberado += archivo.stat().st_size
                        archivo.unlink()
                        eliminados += 1
                        logger.debug(f"Eliminado duplicado: {archivo}")
                except Exception as e:
                    error_msg = f"Error eliminando {archivo_info['nombre']}: {e}"
                    errores.append(error_msg)
                    logger.error(error_msg)
        
        resultado = {
            'exito': True,
            'estrategia_usada': estrategia,
            'archivos_analizados': len(archivos_a_eliminar) + len(archivos_a_conservar),
            'archivos_eliminados': eliminados if confirmar else len(archivos_a_eliminar),
            'archivos_conservados': len(archivos_a_conservar),
            'espacio_liberado': espacio_liberado if confirmar else sum(a['tama√±o'] for a in archivos_a_eliminar),
            'espacio_liberado_legible': self._formatear_bytes(espacio_liberado if confirmar else sum(a['tama√±o'] for a in archivos_a_eliminar)),
            'errores': errores,
            'fue_simulacion': not confirmar
        }
        
        if confirmar:
            logger.info(f"‚úÖ Eliminaci√≥n completada: {eliminados} archivos, {self._formatear_bytes(espacio_liberado)} liberados")
            # Limpiar lista de duplicados encontrados
            self.duplicados_encontrados = []
            self._guardar_duplicados()
        else:
            logger.info(f"üìã Simulaci√≥n: se eliminar√≠an {len(archivos_a_eliminar)} archivos")
        
        return resultado
    
    def obtener_duplicados_manual(self) -> List[Dict[str, Any]]:
        """
        Retorna la lista de duplicados para selecci√≥n manual.
        
        Returns:
            Lista de grupos de duplicados para interfaz manual
        """
        return self.duplicados_encontrados
    
    def eliminar_archivo_especifico(self, ruta_archivo: str) -> bool:
        """
        Elimina un archivo espec√≠fico de duplicados.
        
        Args:
            ruta_archivo: Ruta del archivo a eliminar
            
        Returns:
            True si se elimin√≥ correctamente
        """
        try:
            archivo = Path(ruta_archivo)
            if archivo.exists():
                archivo.unlink()
                logger.info(f"üóëÔ∏è Archivo eliminado manualmente: {archivo.name}")
                return True
            else:
                logger.warning(f"Archivo no encontrado: {ruta_archivo}")
                return False
        except Exception as e:
            logger.error(f"Error eliminando archivo {ruta_archivo}: {e}")
            return False
    
    def _formatear_bytes(self, bytes_size: int) -> str:
        """Formatea bytes en formato legible."""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if bytes_size < 1024.0:
                return f"{bytes_size:.1f} {unit}"
            bytes_size /= 1024.0
        return f"{bytes_size:.1f} PB"
    
    def limpiar_cache(self):
        """Limpia el cache de hashes."""
        self.cache_hashes = {}
        if self.archivo_cache.exists():
            self.archivo_cache.unlink()
        logger.info("üßπ Cache de duplicados limpiado")
    
    def configurar_algoritmo(self, algoritmo: str) -> bool:
        """
        Configura el algoritmo de hash a usar.
        
        Args:
            algoritmo: 'md5' o 'sha256'
            
        Returns:
            True si se configur√≥ correctamente
        """
        if algoritmo not in ['md5', 'sha256']:
            logger.error(f"Algoritmo inv√°lido: {algoritmo}. V√°lidos: md5, sha256")
            return False
        
        if algoritmo != self.algoritmo_hash:
            self.algoritmo_hash = algoritmo
            # Limpiar cache si cambi√≥ el algoritmo
            self.limpiar_cache()
            logger.info(f"üîß Algoritmo de hash cambiado a: {algoritmo}")
        
        return True 